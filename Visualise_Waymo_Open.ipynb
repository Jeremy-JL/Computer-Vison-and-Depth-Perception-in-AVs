{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Install the Waymo Open Dataset Package and import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf waymo-od > /dev/null\n",
    "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
    "!cd waymo-od && git branch -a\n",
    "!cd waymo-od && git checkout remotes/origin/master\n",
    "!pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install waymo-open-dataset-tf-2-1-0==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from waymo_open_dataset.utils import range_image_utils\n",
    "from waymo_open_dataset.utils import transform_utils\n",
    "from waymo_open_dataset.utils import  frame_utils\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Segment\n",
    "\n",
    "Import Waymo Open segment data as .tfrecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILENAME = 'Path to file segment.tfrecord'\n",
    "FILENAME = '/content/validation_segment-12866817684252793621_480_000_500_000_with_camera_labels.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "for data in dataset:\n",
    "    frame = open_dataset.Frame()\n",
    "    frame.ParseFromString(bytearray(data.numpy()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(range_images, camera_projections,range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Scene\n",
    "Visualise the scene within the .tfrecord segment captured by cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_camera_image(camera_image, camera_labels, layout, cmap=None):\n",
    "\n",
    "  ax = plt.subplot(*layout)\n",
    "\n",
    "  # Draw the camera labels.\n",
    "  for camera_labels in frame.camera_labels:\n",
    "    # Ignore camera labels that do not correspond to this camera.\n",
    "    if camera_labels.name != camera_image.name:\n",
    "      continue\n",
    "\n",
    "  # Show the camera image.\n",
    "  plt.imshow(tf.image.decode_jpeg(camera_image.image), cmap=cmap)\n",
    "  plt.title(open_dataset.CameraName.Name.Name(camera_image.name))\n",
    "  plt.grid(False)\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "\n",
    "for index, image in enumerate(frame.images):\n",
    "  show_camera_image(image, frame.camera_labels, [3, 3, index+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidar Point Cloud Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "    frame,\n",
    "    range_images,\n",
    "    camera_projections,\n",
    "    range_image_top_pose)\n",
    "points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
    "    frame,\n",
    "    range_images,\n",
    "    camera_projections,\n",
    "    range_image_top_pose,\n",
    "    ri_index=1)\n",
    "\n",
    "# 3d points in vehicle frame.\n",
    "points_all = np.concatenate(points, axis=0)\n",
    "points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
    "\n",
    "# camera projection corresponding to each point.\n",
    "cp_points_all = np.concatenate(cp_points, axis=0)\n",
    "cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(frame.images, key=lambda i:i.name)\n",
    "cp_points_all_concat = np.concatenate([cp_points_all, points_all], axis=-1)\n",
    "cp_points_all_concat_tensor = tf.constant(cp_points_all_concat)\n",
    "\n",
    "# The distance between lidar points and vehicle frame origin.\n",
    "points_all_tensor = tf.norm(points_all, axis=-1, keepdims=True)\n",
    "cp_points_all_tensor = tf.constant(cp_points_all, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select LiDAR Point Cloud from scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.equal(cp_points_all_tensor[..., 0], images[0].name)\n",
    "\n",
    "\"\"\"Updating images[#] switches lidar point cloud\n",
    "0 = Front\n",
    "1 = Front Left\n",
    "2 = Front Right\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_points_all_tensor = tf.cast(tf.gather_nd(\n",
    "    cp_points_all_tensor, tf.where(mask)), dtype=tf.float32)\n",
    "points_all_tensor = tf.gather_nd(points_all_tensor, tf.where(mask))\n",
    "\n",
    "projected_points_all_from_raw_data = tf.concat(\n",
    "    [cp_points_all_tensor[..., 1:3], points_all_tensor], axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LiDAR Point Cloud Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgba(r):\n",
    "  \n",
    "  c = plt.get_cmap('jet')((r % 75.0) / 50.0)\n",
    "  c = list(c)\n",
    "  c[-1] = 0.8  #Adjust Transparency of Lidar Point Cloud (Alpha)\n",
    "  return c\n",
    "\n",
    "def plot_image(camera_image):\n",
    "\n",
    "  plt.figure(figsize=(20, 12))\n",
    "  plt.imshow(tf.image.decode_jpeg(camera_image.image))\n",
    "  plt.grid(\"off\")\n",
    "\n",
    "def plot_points_on_image(projected_points, camera_image, rgba_func,\n",
    "                         point_size=5.0):\n",
    "  \"\"\"Plots points on a camera image.\n",
    "\n",
    "  Args:\n",
    "    projected_points: [N, 3] numpy array. The inner dims are\n",
    "      [camera_x, camera_y, range].\n",
    "    camera_image: jpeg encoded camera image.\n",
    "    rgba_func: a function that generates a color from a range value.\n",
    "    point_size: the point size.\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  plot_image(camera_image)\n",
    "\n",
    "  xs = []\n",
    "  ys = []\n",
    "  colors = []\n",
    "\n",
    "  for point in projected_points:\n",
    "    xs.append(point[0])  # width, col\n",
    "    ys.append(point[1])  # height, row\n",
    "    colors.append(rgba_func(point[2]))\n",
    "\n",
    "  plt.scatter(xs, ys, c=colors, s=point_size, edgecolors=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_on_image(projected_points_all_from_raw_data,\n",
    "                     images[0], rgba, point_size=10.0)\n",
    "\n",
    "\"\"\"Updating images[#] switches bg image\n",
    "0 = Front\n",
    "1 = Front Left\n",
    "2 = Front Right\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Lidar Point Cloud without Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_points_img(projected_points, rgba_func, point_size=5.0):\n",
    "\n",
    "  xs = []\n",
    "  ys = []\n",
    "  colors = []\n",
    "\n",
    "  for point in projected_points:\n",
    "    xs.append(point[0])  # width, col\n",
    "    ys.append(point[1])  # height, row\n",
    "    colors.append(rgba_func(point[2]))\n",
    "\n",
    "  plt.figure(figsize=(20, 8.5))\n",
    "  plt.scatter(xs, ys, c=colors, s=point_size, edgecolors=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to plot image\n",
    "\n",
    "plt_points_img(projected_points_all_from_raw_data, rgba, point_size=15.0)\n",
    "#plt.grid(\"off\")\n",
    "plt.axis('off')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Image Frame for Monocular Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_image(camera_image):\n",
    "\n",
    "  plt.figure(figsize=(30, 20))\n",
    "  plt.imshow(tf.image.decode_jpeg(camera_image.image))\n",
    "  plt.axis('off')\n",
    "  #plt.grid(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_image(images[0])\n",
    "#plt.savefig('image5.png', bbox_inches='tight',pad_inches = 0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
